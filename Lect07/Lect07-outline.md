# DL4ES: Занятие 7

## Практическое занятие: создание и обучение нейросетей в PyTorch (продолжение)

**Тренировка и валидация нейронных сетей в PyTorch.  
Датасеты, DataLoader, трансформации и аугментация данных.**

---

## 1. Введение
- Краткое напоминание о предыдущей лекции: архитектура полносвязной сети, логиты, функция потерь, цикл обучения.
- Постановка задачи лекции: разобрать *полный цикл обучения* и *валидации* моделей, а также механизмы подготовки данных — `Dataset`, `DataLoader`, трансформации и аугментации.

---

## 2. Цикл обучения нейронной сети
### 2.1. Обучение на одну эпоху
- Перебор всей обучающей выборки батчами.
- Вычисление логитов модели: `output = model(data)`.
- Вычисление функции потерь (CrossEntropyLoss).
- Обратное распространение ошибки: `loss.backward()`.
- Обновление параметров оптимизатором (SGD/Adam).
- Сохранение истории функции потерь (loss history).

### 2.2. Поведение функции потерь
- Стохастический характер градиентов → «шумная» динамика loss.
- Ожидаемое поведение: быстрое снижение, затем насыщение.
- Влияние размера батча и способа оптимизации.

---

## 3. Валидация модели
### 3.1. Принципы валидации
- Параметры модели фиксируются — **без вычисления градиентов**.
- Прогон модели по отложенной (валидационной) выборке.
- Подсчёт:
  - среднего значения функции потерь,
  - accuracy (доля верно определённых классов).

### 3.2. Использование логитов
- Максимум по оси классов → предсказанная метка.
- Причина отсутствия `softmax` на выходе обучения.
- Отличие логитов от вероятностей.

---

## 4. Dataset, DataLoader и трансформации
### 4.1. Класс Dataset
- Хранение коллекции данных (признаки + метки).
- Реализуемые методы:
  - `__len__()`: количество объектов,
  - `__getitem__(i)`: вернуть *i*-й объект (тензор + метка).
- Для MNIST: изображения 28×28 и целые метки 0–9.

### 4.2. DataLoader — генератор батчей
- Принимает Dataset и собирает батчи (`batch_size`).
- Возможность перемешивания (`shuffle=True`).
- Итератор, возвращающий кортеж `(data_batch, target_batch)`.

### 4.3. Трансформации и аугментации
- Используются в `torchvision.transforms`.
- Цели:
  - нормализация данных,
  - преобразование типов (`ToTensor`),
  - искусственное расширение выборки (augmentation),
  - повышение устойчивости модели.

#### 4.3.1. Пример аугментаций для обучения:
- `RandomRotation(-30°, +30°)`,
- `RandomResizedCrop(28×28)` с масштабом (0.5–1),
- нормализация с заранее вычисленными µ и σ.

#### 4.3.2. Трансформации для валидации:
- Только приведение к тензору и нормализация.
- **Никакой аугментации**, чтобы не искажать истинное распределение.

---

## 5. Практическая демонстрация (по ходу лекции)
- Запуск обучения модели.
- Просмотр размера батчей, формы тензоров.
- Просмотр меток и логитов.
- Визуализация эволюции функции потерь.
- Сравнение скорости обучения и валидации.
- Достигаемое качество на MNIST (~95–97% после нескольких эпох).

---

## 6. Обсуждение
- Почему аугментация нужна всегда, даже при больших моделях?
- Что происходит при перепараметризации?
- Почему нейронные сети *не всегда* переобучаются, несмотря на число параметров?
- Как меняется динамика loss при разных размерах батча?

---

## 7. Заключение
- Разобраны ключевые компоненты пайплайна обучения:  
  обучение → валидация → обработка данных.
- Переход к следующей теме: **TensorBoard и визуализация процесса обучения** (следующая лекция).
- Напоминание о дедлайнах по домашней работе.

