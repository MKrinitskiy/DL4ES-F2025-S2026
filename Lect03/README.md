# DL4ES: Лекция 3

## Градиентная оптимизация нейронных сетей: от классики к стохастическим методам

В этой лекции рассматриваются фундаментальные принципы оптимизации параметров нейронных сетей, начиная с классического градиентного спуска на примере линейной регрессии. Подробно обсуждается природа функции потерь, её связь с предположениями о шуме и максимизацией правдоподобия, а также геометрическая интерпретация ландшафта ошибки в пространстве параметров. Через анализ поведения функций MSE и градиентов студенты получают представление о том, как устроены процессы минимизации в простых моделях.

Далее лекция переходит к обсуждению ключевых ограничений полного градиентного спуска и причин, по которым он практически неприменим в обучении современных нейросетей. Особое внимание уделяется влиянию размера выборки на оценку градиента, росту шума при уменьшении подвыборки и проблемам вычислительных ресурсов. На конкретных примерах демонстрируется, как изменение шага обучения влияет на сходимость: от стабильной оптимизации — до зигзагообразных траекторий и полной расходимости.

Основным акцентом второй части является стохастический градиентный спуск (SGD). Он рассматривается как практическая и масштабируемая альтернатива классическому методу при работе с большими данными. В рамках лекции показано, как шумные оценки градиента всё же позволяют уверенно двигаться к минимуму, почему SGD является стандартом де-факто в обучении глубоких моделей и что происходит с ландшафтом функции потерь при использовании случайных подвыборок. Лекция завершает плавный переход к теме следующих занятий — оптимизации в глубоких архитектурах и свойствам их сложных, высокоразмерных ландшафтов ошибок.
