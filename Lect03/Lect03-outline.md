# DL4ES: Лекция 3

## Оптимизация нейронных сетей. Градиентный спуск и стохастическая оптимизация

## 1. Введение
- Краткое напоминание о задаче оптимизации нейросетей.
- Функция потерь как объект минимизации.
- Пространство параметров, параметры модели (пример: линейная регрессия).

## 2. Линейная регрессия как простейшая нейросеть
- Архитектура: взвешенная сумма признаков + смещение.
- Определение функции потерь MSE.
- Происхождение MSE из предположения о нормальном шуме и максимизации правдоподобия.

## 3. Ландшафт функции потерь
- Пример ландшафта для линейной регрессии: выпуклый, единственный минимум.
- Геометрия: пространство параметров и значение функции потерь в каждой точке.

## 4. Аналитическое решение (редкий случай)
- Формула Xθ − y и MSE в матричном виде.
- Градиент, приравнивание градиента к нулю.
- Единственный случай, когда возможно аналитическое решение — линейная регрессия.

## 5. Градиентный спуск
### 5.1. Основная идея
- Градиент указывает направление наискорейшего увеличения.
- Шагаем в сторону антиградиента.
- Итерационный процесс: начальная точка, шаг, критерии остановки.

### 5.2. Learning rate (темп обучения)
- Слишком маленький — медленная сходимость.
- Слишком большой — зигзаги, расходимость.
- Примеры корректной и некорректной эволюции функции потерь.

### 5.3. Эволюция функции потерь
- Как выглядит “классическая” гладкая траектория.
- Случаи, когда функция ошибок растёт → оптимизация разошлась.

## 6. Ограничения базового градиентного спуска
- Невозможность визуализировать ландшафт при высокой размерности.
- Невозможность загрузить полную выборку в память.
- Проблема больших датасетов.

## 7. Идея стохастической оптимизации
### 7.1. Основной принцип
- Градиенты оцениваются по подвыборке данных.
- Ландшафт функции потерь «дрейфует» из-за подвыборок.
- Градиент становится случайной величиной.

### 7.2. Шум в градиентах
- Оценка градиента по 1/2, 1/4, 1/8 выборки.
- Рост дисперсии оценок.
- Понимание, что при достаточно большом количестве шагов шум не мешает сходимости.

### 7.3. Stochastic Gradient Descent (SGD)
- Каждая итерация: новая подвыборка → новый градиент.
- Траектория оптимизации шумная, но сходится.
- Эволюция функции потерь: шум, насыщение вокруг минимума.

### 7.4. Практические аспекты
- Размер мини-батча как гиперпараметр.
- Компромисс между точностью оценки антиградиента и затратами памяти.
- Выбор стратегии подвыборки.

## 8. Сравнение GD и SGD
- GD: стабильный, но требует всю выборку в памяти.
- SGD: шумный, но масштабируемый.
- Реальность: все современные нейросети обучаются SGD и его модификациями.

## 9. Заключение и анонс следующей лекции
- Итоги: градиентный спуск, выбор шага, природа стохастической оптимизации.
- Переход к глубоким нейросетям и их сложным ландшафтам потерь на следующем занятии.
