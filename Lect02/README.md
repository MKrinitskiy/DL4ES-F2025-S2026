# DL4ES: Лекция 2

## Искусственные нейронные сети и градиентные методы оптимизации

В этой лекции были разобраны фундаментальные принципы построения и
обучения моделей глубокого обучения. Мы начали с обсуждения функций
потерь и их связи с вероятностной природой данных, а затем перешли к
обобщённым линейным моделям, показывая, как от простых линейных моделей
возникает необходимость в более выразительных архитектурах. Особое
внимание уделено тому, что любая модель в машинном обучении оценивает не
саму целевую переменную, а параметр распределения, из которого эта
переменная предполагается порождённой.

Затем мы обсудили идею глубины нейросетей: усложнение модели путём
последовательного применения линейных преобразований и функций
активации. Это позволило формализовать понятие искусственной нейронной
сети и показать, что даже такие модели, как линейная и логистическая
регрессия, являются частными случаями однослойных нейросетей.
Дополнительно были введены ключевые элементы архитекутры нейросетей,
включая выбор активаций и правила формирования слоёв.

Заключительная часть лекции была посвящена методам оптимизации и, прежде
всего, градиентному спуску. Мы рассмотрели мотивацию его использования,
свойства ландшафта функции потерь и причины, по которым аналитические
или переборные методы становятся непригодны в высоких размерностях.
Лекция завершилась подготовкой к обсуждению алгоритма обратного
распространения ошибки и автоматического дифференцирования, которые
лежат в основе обучения глубоких нейросетей.
