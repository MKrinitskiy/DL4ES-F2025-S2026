# DL4ES: Лекция 2
## **План‑конспект лекции**

### **Тема лекции:**
Обобщённые линейные модели, искусственные нейронные сети и основы градиентной оптимизации.

---

## 1. **Рекэп предыдущей лекции**
- Функции потерь: происхождение, связь с методом максимального правдоподобия.
- Различие между функцией потерь и мерой качества.
- Потери для регрессии (MSE) и классификации (логистическая/кросс‑энтропия).
- Ключевая мысль: модель всегда оценивает **параметр распределения**, а не саму целевую переменную напрямую.

---

## 2. **Обобщённые линейные модели (GLM)**
- Общая структура: линейная часть + функция активации.
- Линейная регрессия как частный случай GLM с тождественной функцией активации.
- Логистическая регрессия как GLM с сигмоидой.
- Мультиномиальная классификация и softmax:
  - почему softmax — единственная «векторная» функция активации;
  - нормировка вероятностей до единицы;
  - размерности параметров (пример: матрица θ размерности 4×6).

---

## 3. **Функции потерь в GLM**
- Вывод MSE и логистической функции потерь из предположений о распределении.
- Кросс‑энтропия в многоклассовом случае.
- Важность дифференцируемости функции потерь.
- Обсуждение того, что ландшафт функции потерь задаётся данными и архитектурой модели.

---

## 4. **Ограничения аналитической оптимизации**
- Линейная регрессия — почти единственный случай аналитического решения.
- Логистическая регрессия: из-за нелинейности аналитического решения нет.
- Введение понятия *ландшафта функции потерь*.
- Первая иллюстрация выпуклого/невыпуклого ландшафта.

---

## 5. **Классы методов оптимизации**
- **Аналитические методы:** приравнивание градиентов к нулю.
- **Стохастические методы (Монте‑Карло):**
  - идея случайного перебора точек в пространстве параметров;
  - экспоненциальный рост вычислительной сложности.
- **Эволюционные методы:** концепция «популяций решений».
- **Квантовые методы:** ограничения по числу кубитов.
- Подводка: почему нейросети оптимизируются почти всегда **градиентно**.

---

## 6. **Переход к нейросетям**
- Мотивация усложнения моделей: повышение выразительной способности.
- От GLM → к составлению моделей в цепочку.
- Появление идеи глубины модели.

---

## 7. **Искусственные нейронные сети (ANN)**
### 7.1. Общая структура
- Полносвязная нейросеть как последовательность преобразований «линейная часть + активация».
- Формальное определение нейросети через композицию функций.
- Размерности матриц весов.

### 7.2. Терминология
- Слой = линейное преобразование + функция активации.
- Входной слой — *не слой* в современном определении.
- Выходной слой: выбор активации зависит от задачи:
  - регрессия → identity;
  - бинарная классификация → sigmoid;
  - многоклассовая → softmax.

### 7.3. Примеры
- Линейная регрессия как однослойная нейросеть.
- Логистическая регрессия как однослойная нейросеть с сигмоидой.
- Простая трёхслойная сеть и подсчёт числа параметров (пример: 71 параметр).

---

## 8. **Градиентная оптимизация**
### 8.1. Мотивация
- Высокая размерность параметров делает перебор невозможным.
- Градиент указывает направление наискорейшего возрастания функции.
- Для минимизации шагаем **против** градиента.

### 8.2. Понятие градиента
- Градиент как вектор частных производных.
- Градиент функции потерь для нейросети можно вычислить аналитически (через цепное правило).

### 8.3. Метод градиентного спуска
- Итерационное обновление параметров: θ_{k+1} = θ_k − η ∇L(θ_k).
- **Learning rate (темп обучения):** влияние на шаг.
- **Начальное приближение:** почему нейросети его выбирают случайно.
- **Критерии остановки:**
  - малое изменение потерь;
  - малое изменение градиента;
  - ограничение по числу итераций.

---

## 9. **Демонстрация: линейная регрессия и визуализация ландшафта потерь**
- Трёхмерная поверхность MSE.
- Связь между θ и формой плоскости регрессии.
- Наглядное объяснение того, почему у квадратичной функции один глобальный минимум.
- Визуализация движения по ландшафту градиентным методом.

---

## 10. **Финальная часть лекции**
- Подготовка к теме следующей лекции:
  - автоматическое дифференцирование;
  - вычисление градиентов в нейросетях (backpropagation);
  - реализация ручного градиентного спуска в коде.
- Анонс домашнего задания (будет позже): реализовать вычисление градиентов вручную.

---

## **Аннотация (короткое резюме)**
На лекции были рассмотрены обобщённые линейные модели, связь функций потерь с правдоподобием и подходом GLM, введён формальный аппарат искусственных нейронных сетей и терминология слоёв. Подробно разобрано понятие ландшафта функции потерь и мотивация градиентных методов оптимизации. Рассмотрены методы оптимизации, их ограничения и причины, по которым нейросети обучаются именно градиентно. Завершили лекцию обсуждением структуры многослойных сетей и подготовкой к изучению алгоритма обратного распространения ошибки.

