# DL4ES: Лекция 5

## Построение вычислительного графа и базовое автодифференцирование

В этой лекции студенты познакомились с фундаментальными принципами построения вычислительных графов и ручной реализации автоматического дифференцирования на основе классов в Python. На примере простой линейной регрессии была разобрана логика прямого и обратного проходов, а также показано, как формируются градиенты по аргументам каждой операции. Лекция дала целостное понимание того, как устроены внутренние механизмы современных фреймворков глубокого обучения.

Особое внимание было уделено структуре классов `forward`, `backward` и роли хранения промежуточных значений, необходимых для вычисления градиентов. Студенты научились строить вычислительный граф из отдельных «кирпичиков» — линейного слоя, функции активации и операций функции потерь. Разбор корреляций между признаками и влияния мультиколлинеарности на форму ландшафта функции потерь позволил связать концепции оптимизации с предварительным анализом данных.

В заключение лекции были разобраны примеры сборки модели, вычисления градиента функции потерь и выполнения шага градиентного спуска. Такой низкоуровневый подход формирует прочную базу для последующих занятий, где студенты будут переходить к реализации многослойных нейронных сетей и осваивать более сложные архитектуры, опираясь на понимание механики автодифференцирования.

