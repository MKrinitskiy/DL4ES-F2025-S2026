# DL4ES: Лекция 5

## Построение вычислительного графа и базовое автодифференцирование

**Дата:** 14 октября 2025  
**Тема:** Реализация автоматического дифференцирования и линейной регрессии на NumPy

---

## 1. Организационная часть
- Проверка связи с онлайн‑участниками.  
- Краткий разбор вопросов по Python (классы, методы).  
- Напоминание о структуре и цели текущего домашнего задания.

---

## 2. Цель занятия
- Понять, как устроена реализация вычислительного графа «вручную».  
- Научиться строить forward‑ и backward‑проходы без PyTorch.  
- Реализовать линейную регрессию как простейшую нейросеть с одной линейной операцией.  
- Освоить использование классов как «кирпичиков» для построения вычислительного графа.

---

## 3. Исходные данные и их структура
- Признаковое описание: 4 признака на каждый объект.  
- Целевая переменная: скаляр.  
- Задача: выполнить анализ данных, визуализацию и отфильтровать скоррелированные признаки.

### 3.1 Поиск коррелированных признаков
- Построение диаграмм рассеяния между \(x_i\) и \(y\).  
- Вычисление матрицы корреляций (NumPy).  
- Решение об удалении признаков с высокой корреляцией.  
- Мотивация: коррелированные признаки → плохой ландшафт функции потерь → медленная и нестабильная оптимизация.

---

## 4. Линейная регрессия как «нейросеть»
Модель:
\[
z = 	\theta^T\cdot x,\quad \mu = 	\mathbb{I}(z)
\]

Состоит из:
1. Линейного слоя: `Linear`  
2. Функции активации: `Identity`  

---

## 5. Реализация вычислительного графа через классы
Каждый элемент графа — отдельный класс, наследник `Differentiable`.  

### 5.1 Общая структура класса
- `__init__` — создание экземпляра и атрибутов.  
- `forward()` — прямой проход: вычисление результата операции.  
- `backward(upstream_grad)` — вычисление градиента по аргументам.  
- `__call__()` — синтаксический сахар, вызывает `forward()`.

### 5.2 Linear
Операция:  
\[
z = 	\theta^T\cdot x
\]

Хранит:  
- веса \(\theta\);  
- входы для использования в backward.  

Градиенты:  
\[
rac{\partial L}{\partial 	heta}, \quad rac{\partial L}{\partial x}
\]

### 5.3 Identity
\[
\mathbb{I}(x) = x
\]

Backward просто передает upstream‑gradient.

---

## 6. Функция потерь
Используем MSE:
\[
\mathcal{L} = \sum (\mu - y)^2
\]

Рекомендуемая реализация через два класса:
1. `Squared` — возведение в квадрат  
2. `Sum` — суммирование  

Backward должен вернуть:
- градиент по μ  
- градиент по \(y\) (не используется далее, но должен быть реализован)

Размерность градиента совпадает с размерностью аргумента — правило для проверки.

---

## 7. Обратное распространение
Общий принцип:

1. У функции потерь `backward(1.0)` → получить  
   - градиент по μ  
   - градиент по y  

2. Градиент по μ передается в backward функции активации.  
3. Далее — в backward линейного слоя.  
4. Получаем \(\partial L / \partial 	heta\).

---

## 8. Обучение модели
Алгоритм на каждую эпоху:

1. Forward:  
   \[
   \hat{y} = 	\operatorname{Network}(X)
   \]
2. Вычисление функции потерь.  
3. Backward — получение градиента.  
4. Шаг градиентного спуска:  
   \[
   \theta^{(t+1)} := 	\theta^{(t)} - \eta \cdot \frac{\partial \mathcal{L}}{\partial 	\theta}
   \]
5. Запись истории значения функции потерь.  

---

## 9. Визуализация результатов
- График истории значения функции потерь.  
- Диаграмма рассеяния «истинное значение vs предсказанное».  

---

## 10. Резюме лекции
- Реализована структура вычислительного графа.  
- Рассмотрена логика forward/backward без фреймворков.  
- Подготовлена основа для реализации многослойных сетей.  
- Домашнее задание: полностью собрать рабочую систему автодифференцирования.

