# DL4ES 2025: Лекция 9

## Импульсный метод, скрытые представления и шум в оптимизации нейросетей

---

## 1. Организационная часть
- Напоминание: последние занятия были посвящены структуре обучающего цикла, мониторингу, логированию, работе с TensorBoard.

---

## 2. Завершение темы TensorBoard: распределения и гистограммы
### 2.1. Повторение базовых элементов
- Режимы модели (`train()`, `eval()`).
- Итерация по батчам.
- Вычисление выхода модели.
- Подсчёт функции потерь.
- .backward() и шаг оптимизатора.

### 2.2. Что уже логируем
- Скалярные метрики по эпохам.

### 2.3. Добавление гистограмм
- Гистограммы весов и смещений всех слоёв.
- Гистограммы градиентов.
- Почему слои нумеруются 0,2,4,6,8.

### 2.4. Что видно в TensorBoard
- Эволюция распределений.
- Особенности разных слоёв.

---

## 3. Скрытые представления
- Что такое Z и H.
- Варианты трактовки скрытого представления.
- Размерности H и Z.
- Градиенты можно считать по любому узлу графа.

---

## 4. Мини-ДЗ
- Добавить в TensorBoard гистограммы скрытых представлений.
- Варианты реализации: ручное прописывание или PyTorch hooks.
- Что сдавать: ноутбук + TensorBoard-логи.

---

## 5. Градиентная оптимизация
### 5.1. Полный градиентный спуск
- Использует всю выборку.

### 5.2. Стохастический градиентный спуск
- Мини-батчи.
- Градиент в точке — случайная величина.
- Ландшафт «дрейфует» между итерациями.

---

## 6. Вытянутые долины
- Причины: масштабы признаков, корреляции (в линейной модели).
- SGD может давать осцилляции.

---

## 7. Импульсный метод
- Экспоненциальное сглаживание градиентов.
- Формула momentum.
- Устранение осцилляций в вытянутых долинах.

---

## 8. Когда моментум не работает
- Сильно разные распределения в батчах.
- Пример — GANs: осреднение делает градиент неинформативным.
- В обычном supervised-обучении наоборот — батч должен быть репрезентативным.

---

## Анонс следующей лекции
- RMSProp, Adam, свойства loss landscape современных моделей.
