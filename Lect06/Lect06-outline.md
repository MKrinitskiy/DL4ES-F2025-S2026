# DL4ES: Занятие 6

## Практическое построение и обучение нейросетей в PyTorch

Практическое введение в построение, компоновку и обучение искусственных нейронных сетей в фреймворке PyTorch.

## Основные цели занятия
- Понять структуру и логику практической реализации нейросетей.
- Научиться создавать и настраивать архитектуру ИНС в коде.
- Освоить базовые этапы обучения: прямой проход, вычисление функции потерь, обратное распространение ошибки, шаг оптимизации.
- Разобраться в механике батчей, эпох, сэмплирования и их связи со стохастической оптимизацией.
- Применить полученные знания на синтетической задаче.

---

## 1. Введение
- Краткое описание целей текущего и следующего занятия.
- Объяснение, почему используется синтетическая задача: акцент на механике, а не на содержании данных.
- Организационная информация: структура практики, связь с домашними заданиями.

---

## 2. Фреймворки для нейросетей
Обзор современных библиотек:
- PyTorch (основной инструмент курса).
- JAX.
- TensorFlow / Keras.
- MindSpore.
- Российские аналоги (упоминание о ранних проектах).
Пояснение о том, почему PyTorch выбран в курсе.

---

## 3. Обзор механики обучения нейросетей

### 3.1. Итерационная оптимизация
На каждой итерации выполняется:
1. Прямой проход (forward): вычисление выходов сети.
2. Вычисление функции потерь.
3. Вычисление градиентов функции потерь по параметрам (backward).
4. Шаг оптимизации.
5. Логирование значений (loss, метрики).
6. Проверка условий остановки.

### 3.2. Эпохи и батчи
- Итерация ≠ эпоха.
- Батч — мини-порция данных.
- Эпоха — полное прохождение по тренировочной выборке.
- Поведение в случае "бесконечных" датасетов и стриминговых данных.
- Мини‑батч стохастическая оптимизация.

### 3.3. Сэмплирование данных
- Что такое сэмплирование.
- Сэмплирование из выборки как приближение сэмплирования из распределения.
- Разбор стратегий: без возвращения, с возвращением (bootstrap — кратко упомянуть).

---

## 4. Архитектура нейросети в PyTorch

### 4.1. Что такое `nn.Module`
- Базовый класс всех моделей.
- Способ организации параметров и операций.

### 4.2. Создание собственного класса нейросети
- Описание слоев в `__init__`.
- Понимание того, как создаются параметры (матрицы θ).
- Важность явного объявления размерностей входа и выхода.
- Генерация архитектуры в цикле (гибкость и переиспользуемость).

### 4.3. Линейные слои (`nn.Linear`)
- Задание размерности входа/выхода.
- Автоматическое создание параметров.
- Пояснение: bias реализован отдельно, не через добавление единичного признака.

### 4.4. Функции активации
- Зачем нужна нелинейность.
- Разбор ReLU: быстрый, простой, но негладкий.
- Альтернативы: GELU, Mish, SiLU.
- Почему **softmax не ставят** в конце: строгая монотонность, отсутствие необходимости для градиентной оптимизации.

### 4.5. Сборка слоев в последовательную модель
- Использование `nn.Sequential`.
- Почему важно сохранять слои в атрибуте модуля (иначе не попадут в граф вычисления).

### 4.6. Forward‑метод
- Преобразование входных данных (reshape).
- Последовательное применение слоев.
- Возврат логитов.

---

## 5. Тензоры в PyTorch

### 5.1. Отличия от NumPy
- Наличие вычислительного графа.
- Атрибут `requires_grad`.
- Хранение градиентов в `.grad`.

### 5.2. Какие тензоры имеют градиенты
- Параметры сети — всегда.
- Входные данные — по умолчанию нет.
- Промежуточные вычисления — автогенерируются.

### 5.3. Зачем иногда считают градиенты по входу
- Карты внимания (saliency maps).
- Интерпретация модели в задачах CV и в геофизике.

---

## 6. Создание экземпляра модели
- Практическое построение сети:
  - Задание списка скрытых слоёв: `[512, 256, 128, 32]`
  - Создание экземпляра модели.
  - Подсчёт общего числа параметров (пример: ~570k).

---

## 7. Пример запуска обучения (краткий разбор кода)
- Загрузка данных MNIST (или синтетической выборки).
- Настройка оптимизатора (`SGD`, `Adam`).
- Организация цикла обучения:
  - размер батча;
  - число эпох;
  - логирование;
  - валидация после каждой эпохи.

---

## 8. Домашнее задание и дальнейшие шаги
- Повторить пример из ноутбука.
- Реализовать визуализацию динамики функции потерь.
- Реализовать диаграмму рассеяния "референтные значения vs аппроксимированные".
- Понять разницу между GD и SGD.
- Подготовиться к следующему занятию — разбору полного цикла обучения и оптимизаторов.

---

## Завершение
- Ответы на вопросы.
- Обсуждение возможных проблем при установке PyTorch.
- Настройка на практическую работу с ноутбуком.

