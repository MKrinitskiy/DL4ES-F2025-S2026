# DL4ES: Лекция 4

## Тема лекции
**Метод обратного распространения ошибки (Backpropagation): вычисление градиентов в нейронных сетях**

---

## 1. Введение
- Напоминание о предыдущих лекциях: градиентный спуск, стохастический градиентный спуск, функция потерь.  
- Особенность SGD: оценка градиента по подвыборкам → «дрожащий» ландшафт функции потерь.  
- Почему градиент является случайной величиной при использовании SGD.  

---

## 2. Формальная постановка задачи градиента
- Функция потерь как функция от трёх аргументов:  
  1. Параметры модели θ  
  2. Признаки X  
  3. Референсные значения Y  
- Градиент функции потерь по параметрам как основной инструмент оптимизации.  
- Необходимость вычислять частные производные по параметрам модели.

---

## 3. Построение вычислительного графа
- Представление вычислений модели в виде направленного графа.  
- Промежуточные переменные и узлы операций.  
- Пример простой функции:  
  - Операции сложения, вычитания, умножения.  
  - Явное построение графа и цепочка зависимостей.  
- Роль вычислительного графа:  
  - определение порядка вычисления прямого прохода;  
  - упрощение обратного прохода.

---

## 4. Forward pass  
**Цель:** вычислить значение функции и сохранить промежуточные результаты (cache).  
- Сначала вычисляем все промежуточные величины.  
- Всё, что понадобиться при вычислении градиента, должно быть сохранено.  
- Пример: вычисление E, F, G, H и сохранение значений.

---

## 5. Backward pass — метод обратного распространения ошибки
### 5.1. Основная идея
- Градиент вычисляется *задом наперёд* по графу.  
- Использование **цепного правила**:  
  \[
  \frac{\partial H}{\partial A} = \frac{\partial H}{\partial G}\cdot \frac{\partial G}{\partial E}\cdot \frac{\partial E}{\partial A}
  \]

### 5.2. Upstream‑градиент
- Понятие upstream gradient:  
  это градиент, приходящий **сверху** по графу (от следующих операций).  
- Каждая операция должна:
  1. получить upstream‑градиент;  
  2. умножить его на локальную производную;  
  3. передать дальше вниз.

### 5.3. Структура кода будущих операций
Для каждой операции нужно реализовать:  
- прямой проход (forward): вычисление результата и сохранение cache;  
- обратный проход (backward):  
  - приём upstream‑градиента,  
  - вычисление градиентов по аргументам.

---

## 6. Усложнённый пример вычислений
- Добавление нелинейности (квадрат).  
- Несколько путей в графе → суммирование градиентов.  
- Пошаговая ручная демонстрация вычислений.

---

## 7. Применение backpropagation к линейной регрессии
### 7.1. Вычислительный граф линейной регрессии
- Перемножение θ и X → μ  
- Вычитание Y → Δ  
- Возведение Δ в квадрат  
- Суммирование ошибок по объектам  
- Получение функции потерь L  

### 7.2. Операции, которые нужно реализовать
1. Умножение матриц/векторов  
2. Вычитание  
3. Квадрат  
4. Суммирование  
5. (Неявно) тождественная активация  

### 7.3. Градиент функции потерь по θ
- Итоговая цель: получить $\frac{\partial L}{\partial \theta}  $
- Использование цепного правила и промежуточных значений μ и Δ  

---

## 8. Замечания о реализации
- Хранение промежуточных значений только для текущего объекта — cache.  
- Все операции должны быть написаны как независимые «кубики».  
- Классический backpropagation — это не единая формула, а последовательность локальных вычислений.  

---

## 9. Дальнейшая работа
- Домашнее задание: реализовать backpropagation для линейной регрессии **вручную**.  
- Первое приближение θ: генерация из N(0,1).  
- Подготовка ко второй части задания: многоуровневая сеть и нетривиальная активация.

---

