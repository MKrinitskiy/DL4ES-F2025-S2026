# DL4ES: Лекция 4

## Обратное распространение ошибки: ключевой механизм обучения нейросетей

В этой лекции мы разобрали фундаментальный принцип, лежащий в основе всех современных методов обучения нейронных сетей — метод обратного распространения ошибки (backpropagation). Отталкиваясь от идеи вычисления функции потерь, мы построили вычислительный граф, позволяющий формально описать последовательность операций, которые выполняет модель в процессе прямого прохода. Это позволило выделить промежуточные значения, необходимые для анализа чувствительности результата по отношению к параметрам модели.

Затем мы подробно рассмотрели механизм обратного прохода — систематического распространения градиентов от выхода модели к её параметрам. Через использование цепного правила и концепции *upstream*-градиента было показано, что любая сложная вычислительная схема разбивается на простые локальные операции, каждая из которых имеет собственную функцию прямого и обратного вычисления. Это делает метод универсальным и применимым к любым архитектурам нейронных сетей.

Наконец, мы применили изученный механизм к задаче линейной регрессии, построив граф вычисления её функции потерь и показав, как backpropagation позволяет получить градиент по параметрам θ. Это стало отправной точкой для будущих домашних заданий, где студенты реализуют собственную систему автоматического дифференцирования и закрепят понимание внутренней механики фреймворков глубокого обучения.

